{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c02a9c0b",
   "metadata": {},
   "source": [
    "\n",
    "# EDA de Supuestos para Detección de Anomalías en Consumo de Gas\n",
    "\n",
    "Este cuaderno evalúa **distribución**, **estacionalidad** y **correlación** para ayudarte a elegir el modelo adecuado de **detección de anomalías** en el consumo comercial e industrial de gas.\n",
    "\n",
    "> Archivo por defecto: `/mnt/data/dataset_contugas.xlsx` (puedes cambiar la ruta en la celda de parámetros).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623cbd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================\n",
    "# Parámetros del análisis\n",
    "# ============================\n",
    "FILE_PATH = \"/mnt/data/dataset_contugas.xlsx\"  # <-- Cambia esta ruta si es necesario\n",
    "SHEET_NAME = None  # None = primera hoja; o pon el nombre/índice de la hoja\n",
    "\n",
    "# ============================\n",
    "# Importación de librerías\n",
    "# ============================\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "from scipy import stats\n",
    "from statsmodels.api import qqplot\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.stattools import jarque_bera\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 140)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0efb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================================\n",
    "# Funciones auxiliares (detección de columnas, etc.)\n",
    "# =========================================\n",
    "\n",
    "def _likely_date_names():\n",
    "    return [\n",
    "        \"fecha\", \"date\", \"fecha_consumo\", \"periodo\", \"period\", \"time\", \"timestamp\",\n",
    "        \"Fecha\", \"FECHA\", \"Date\", \"DATE\"\n",
    "    ]\n",
    "\n",
    "def _likely_target_names():\n",
    "    return [\n",
    "        \"consumo\", \"consumption\", \"gas\", \"m3\", \"m^3\", \"volumen\", \"volume\", \"kwh\",\n",
    "        \"energia\", \"energy\", \"demanda\", \"usage\", \"usg\", \"consumo_m3\", \"consumo_kwh\"\n",
    "    ]\n",
    "\n",
    "def find_date_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    # 1) Intento por nombre\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    for name in _likely_date_names():\n",
    "        if name.lower() in cols_lower:\n",
    "            return cols_lower[name.lower()]\n",
    "    # 2) Intento por tipo: buscar columna que parsea a datetime con baja tasa de error\n",
    "    best_col = None\n",
    "    best_ok = -1\n",
    "    for c in df.columns:\n",
    "        s = df[c]\n",
    "        try:\n",
    "            parsed = pd.to_datetime(s, errors=\"coerce\", dayfirst=True, infer_datetime_format=True)\n",
    "            ok = parsed.notna().sum()\n",
    "            if ok > best_ok and ok >= max(3, int(0.4*len(s))):  # al menos 40% parseable\n",
    "                best_ok = ok\n",
    "                best_col = c\n",
    "        except Exception:\n",
    "            continue\n",
    "    return best_col\n",
    "\n",
    "def find_target_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    # 1) Por nombre\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    for name in _likely_target_names():\n",
    "        for c_lower, c in cols_lower.items():\n",
    "            if name in c_lower:\n",
    "                # Debe ser numérico\n",
    "                if pd.api.types.is_numeric_dtype(df[c]):\n",
    "                    return c\n",
    "    # 2) Elegir la columna numérica con mayor varianza positiva\n",
    "    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    if not num_cols:\n",
    "        return None\n",
    "    var = df[num_cols].var(numeric_only=True).fillna(0.0)\n",
    "    if var.empty:\n",
    "        return None\n",
    "    return var.sort_values(ascending=False).index[0]\n",
    "\n",
    "def infer_seasonal_period(series: pd.Series, max_lag: int = 400) -> Optional[int]:\n",
    "    # Usa autocorrelación simple para detectar el primer pico significativo\n",
    "    # Normaliza la serie y calcula ACF manualmente para evitar dependencias extra\n",
    "    s = series.dropna().values\n",
    "    if len(s) < 20:\n",
    "        return None\n",
    "    s = (s - np.mean(s)) / (np.std(s) if np.std(s) != 0 else 1)\n",
    "    acf_vals = [1.0]\n",
    "    L = min(max_lag, len(s) - 1)\n",
    "    for lag in range(1, L+1):\n",
    "        v = np.corrcoef(s[:-lag], s[lag:])[0,1] if len(s) - lag > 1 else np.nan\n",
    "        acf_vals.append(v)\n",
    "    acf_vals = np.array(acf_vals, dtype=float)\n",
    "\n",
    "    # Detectar picos simples sobre umbral\n",
    "    # Umbral heurístico\n",
    "    thr = 0.25\n",
    "    peaks = [i for i in range(1, len(acf_vals)) if acf_vals[i] is not np.nan and acf_vals[i] > thr]\n",
    "    if peaks:\n",
    "        return peaks[0]\n",
    "    return None\n",
    "\n",
    "def safe_log_transform(x: pd.Series) -> pd.Series:\n",
    "    # Log para positivos, de lo contrario devuelve original\n",
    "    if (x > 0).all():\n",
    "        return np.log(x)\n",
    "    return x\n",
    "\n",
    "def describe_normality(x: pd.Series, alpha: float = 0.05, sample_n: int = 5000) -> dict:\n",
    "    x = x.dropna()\n",
    "    n = len(x)\n",
    "    if n == 0:\n",
    "        return {\"n\": 0}\n",
    "\n",
    "    # Muestras para Shapiro (máx 5000 recomendado)\n",
    "    xs = x.sample(min(sample_n, n), random_state=42) if n > sample_n else x\n",
    "\n",
    "    out = {\"n\": n}\n",
    "    try:\n",
    "        W, p = stats.shapiro(xs)\n",
    "        out[\"shapiro_W\"] = float(W)\n",
    "        out[\"shapiro_p\"] = float(p)\n",
    "        out[\"shapiro_normal\"] = bool(p > alpha)\n",
    "    except Exception as e:\n",
    "        out[\"shapiro_error\"] = str(e)\n",
    "\n",
    "    try:\n",
    "        k2, p = stats.normaltest(x)  # D'Agostino\n",
    "        out[\"dagostino_K2\"] = float(k2)\n",
    "        out[\"dagostino_p\"] = float(p)\n",
    "        out[\"dagostino_normal\"] = bool(p > alpha)\n",
    "    except Exception as e:\n",
    "        out[\"dagostino_error\"] = str(e)\n",
    "\n",
    "    try:\n",
    "        jb_stat, jb_p, _, _ = jarque_bera(x)\n",
    "        out[\"jarque_bera\"] = float(jb_stat)\n",
    "        out[\"jarque_bera_p\"] = float(jb_p)\n",
    "        out[\"jarque_bera_normal\"] = bool(jb_p > alpha)\n",
    "    except Exception as e:\n",
    "        out[\"jarque_bera_error\"] = str(e)\n",
    "\n",
    "    try:\n",
    "        ad = stats.anderson(x, dist=\"norm\")\n",
    "        out[\"anderson_stat\"] = float(ad.statistic)\n",
    "        out[\"anderson_crit\"] = [float(v) for v in ad.critical_values]\n",
    "        out[\"anderson_sig\"] = [float(v) for v in ad.significance_level]\n",
    "        # No hay p-value directo en Anderson; interpretamos por niveles críticos\n",
    "    except Exception as e:\n",
    "        out[\"anderson_error\"] = str(e)\n",
    "\n",
    "    out[\"skew\"] = float(stats.skew(x))\n",
    "    out[\"kurtosis\"] = float(stats.kurtosis(x, fisher=True))\n",
    "    return out\n",
    "\n",
    "def adf_kpss_tests(series: pd.Series, alpha: float = 0.05) -> dict:\n",
    "    res = {}\n",
    "    x = series.dropna().values\n",
    "    if len(x) < 20:\n",
    "        return {\"error\": \"Serie demasiado corta para pruebas ADF/KPSS\"}\n",
    "    try:\n",
    "        adf_stat, adf_p, _, _, adf_crit, _ = adfuller(series.dropna(), autolag=\"AIC\")\n",
    "        res[\"adf_stat\"] = float(adf_stat)\n",
    "        res[\"adf_p\"] = float(adf_p)\n",
    "        res[\"adf_stationary_at_alpha\"] = bool(adf_p < alpha)\n",
    "        res[\"adf_crit\"] = {k: float(v) for k, v in adf_crit.items()}\n",
    "    except Exception as e:\n",
    "        res[\"adf_error\"] = str(e)\n",
    "\n",
    "    try:\n",
    "        kpss_stat, kpss_p, _, kpss_crit = kpss(series.dropna(), regression=\"c\", nlags=\"auto\")\n",
    "        res[\"kpss_stat\"] = float(kpss_stat)\n",
    "        res[\"kpss_p\"] = float(kpss_p)\n",
    "        res[\"kpss_stationary_at_alpha\"] = bool(kpss_p > alpha)  # KPSS: H0 es estacionariedad\n",
    "        res[\"kpss_crit\"] = {k: float(v) for k, v in kpss_crit.items()}\n",
    "    except Exception as e:\n",
    "        res[\"kpss_error\"] = str(e)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee9284d",
   "metadata": {},
   "source": [
    "## 1) Carga y preparación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab4d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Carga del Excel\n",
    "path = Path(FILE_PATH)\n",
    "assert path.exists(), f\"No se encontró el archivo: {path}\"\n",
    "\n",
    "df = pd.read_excel(path, sheet_name=SHEET_NAME)\n",
    "print(\"Dimensiones:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35b25da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detección automática de columna de fecha y variable objetivo (consumo)\n",
    "date_col = find_date_column(df)\n",
    "target_col = find_target_column(df)\n",
    "\n",
    "print(\"Columna de fecha detectada:\", date_col)\n",
    "print(\"Columna de consumo/objetivo detectada:\", target_col)\n",
    "\n",
    "# Conversión de fecha e indexado\n",
    "if date_col is not None:\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\", dayfirst=True, infer_datetime_format=True)\n",
    "    df = df.sort_values(by=date_col)\n",
    "    df = df.set_index(date_col)\n",
    "\n",
    "# Limpieza básica de la columna objetivo\n",
    "if target_col is not None:\n",
    "    df[target_col] = pd.to_numeric(df[target_col], errors=\"coerce\")\n",
    "\n",
    "print(\"Valores nulos por columna:\")\n",
    "print(df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494e2a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Vista rápida de la serie objetivo\n",
    "if target_col is None:\n",
    "    raise ValueError(\"No se detectó una columna objetivo numérica. Revisa los nombres o ajusta la heurística.\")\n",
    "\n",
    "series = df[target_col].dropna()\n",
    "print(\"Cantidad de observaciones:\", len(series))\n",
    "display(series.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af97aad",
   "metadata": {},
   "source": [
    "## 2) Distribución y supuestos de normalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c4ea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Histograma (una figura)\n",
    "plt.figure()\n",
    "series.plot(kind=\"hist\", bins=30, edgecolor=\"black\", alpha=0.7)\n",
    "plt.title(\"Histograma del consumo\")\n",
    "plt.xlabel(target_col)\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc368a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Boxplot (una figura)\n",
    "plt.figure()\n",
    "plt.boxplot(series.dropna().values, vert=True, labels=[target_col])\n",
    "plt.title(\"Boxplot del consumo\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b7c973",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# QQ-plot (una figura)\n",
    "plt.figure()\n",
    "qqplot(series.dropna(), line='s')\n",
    "plt.title(\"QQ-plot vs Normal\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb18a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pruebas de normalidad y momentos\n",
    "norm_info = describe_normality(series)\n",
    "print(\"Pruebas de normalidad y momentos:\")\n",
    "for k, v in norm_info.items():\n",
    "    print(f\"- {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f6df06",
   "metadata": {},
   "source": [
    "## 3) Estacionalidad, tendencia y estacionariedad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa0cbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Serie temporal\n",
    "if isinstance(df.index, pd.DatetimeIndex):\n",
    "    plt.figure()\n",
    "    series.plot()\n",
    "    plt.title(\"Serie temporal del consumo\")\n",
    "    plt.xlabel(\"Fecha\")\n",
    "    plt.ylabel(target_col)\n",
    "    plt.show()\n",
    "\n",
    "    # Pruebas de estacionariedad\n",
    "    print(\"\\nResultados ADF y KPSS:\")\n",
    "    tests = adf_kpss_tests(series)\n",
    "    for k, v in tests.items():\n",
    "        print(f\"- {k}: {v}\")\n",
    "\n",
    "    # ACF y PACF\n",
    "    plt.figure()\n",
    "    plot_acf(series.dropna(), lags=40)\n",
    "    plt.title(\"ACF - Autocorrelación\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plot_pacf(series.dropna(), lags=40, method=\"ywm\")\n",
    "    plt.title(\"PACF - Autocorrelación parcial\")\n",
    "    plt.show()\n",
    "\n",
    "    # Descomposición (periodo inferido)\n",
    "    inferred_period = infer_seasonal_period(series)\n",
    "    print(\"\\nPeriodo estacional inferido (heurístico):\", inferred_period)\n",
    "\n",
    "    # Intentar log si aplica\n",
    "    series_for_decomp = safe_log_transform(series)\n",
    "\n",
    "    if inferred_period is not None and inferred_period >= 2:\n",
    "        try:\n",
    "            result = seasonal_decompose(series_for_decomp.dropna(), period=inferred_period, model=\"additive\", extrapolate_trend=\"freq\")\n",
    "            # Observada\n",
    "            plt.figure()\n",
    "            result.observed.plot()\n",
    "            plt.title(\"Descomposición - Observado\")\n",
    "            plt.show()\n",
    "            # Tendencia\n",
    "            plt.figure()\n",
    "            result.trend.plot()\n",
    "            plt.title(\"Descomposición - Tendencia\")\n",
    "            plt.show()\n",
    "            # Estacional\n",
    "            plt.figure()\n",
    "            result.seasonal.plot()\n",
    "            plt.title(\"Descomposición - Estacionalidad\")\n",
    "            plt.show()\n",
    "            # Residual\n",
    "            plt.figure()\n",
    "            result.resid.plot()\n",
    "            plt.title(\"Descomposición - Residual\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(\"Error en descomposición estacional:\", e)\n",
    "    else:\n",
    "        print(\"No se pudo inferir un periodo confiable para descomposición.\")\n",
    "else:\n",
    "    print(\"El índice no es de tipo fecha. Convierte una columna a fecha y vuelve a ejecutar.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a972be",
   "metadata": {},
   "source": [
    "## 4) Correlación entre variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446b247",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Matriz de correlación entre variables numéricas\n",
    "num_df = df.select_dtypes(include=[np.number]).dropna(how=\"all\")\n",
    "if num_df.shape[1] >= 2:\n",
    "    corr = num_df.corr(numeric_only=True)\n",
    "    print(\"Matriz de correlación (numérica):\")\n",
    "    display(corr)\n",
    "\n",
    "    # Mapa de calor simple (una figura, sin seaborn)\n",
    "    plt.figure()\n",
    "    plt.imshow(corr.values, aspect=\"auto\")\n",
    "    plt.xticks(range(corr.shape[1]), corr.columns, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(corr.shape[0]), corr.index)\n",
    "    plt.title(\"Mapa de calor de correlación\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay suficientes columnas numéricas para una matriz de correlación.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da45421",
   "metadata": {},
   "source": [
    "## 5) Sugerencias de modelado (guía)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7633f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\"\"\n",
    "Guía rápida (según supuestos observados):\n",
    "\n",
    "1) Si la distribución es ~normal y sin estacionalidad marcada:\n",
    "   - Z-score, modelos paramétricos, regresión lineal/gaussiana con residuos.\n",
    "\n",
    "2) Si NO hay normalidad (sesgo fuerte) pero la dinámica temporal es importante:\n",
    "   - Modelos robustos/no paramétricos: Isolation Forest, LOF, DBSCAN (con features de tiempo).\n",
    "   - Modelos por residuos: Ajusta ARIMA/SARIMA/Prophet/LSTM y detecta anomalías en el residual.\n",
    "\n",
    "3) Si hay estacionalidad clara (picos en ACF o descomposición):\n",
    "   - SARIMA/Prophet. Detecta anomalías sobre el residual (valores que exceden umbrales en residuales).\n",
    "\n",
    "4) Si existen múltiples variables explicativas (clima, categoría, ubicación, etc.):\n",
    "   - Modelos multivariados: regresión robusta, random forest, XGBoost o autoencoders multivariados.\n",
    "   - Detección de anomalías en el espacio de error de predicción.\n",
    "\n",
    "5) Si la serie no es estacionaria (ADF > 0.05 y/o KPSS < 0.05):\n",
    "   - Diferenciación (d=1) o transformaciones (log) antes de modelar.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
